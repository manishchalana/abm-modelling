{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we implement the concept of structural stochastic volatility which derives from different noise levels in the demand of fundamentalist and chartists and time varying market share of the two groups. \n",
    "\n",
    "We consider the || name here || approach of endogenous switching between the trading strategies and estimate the model by method of simulated moments. where choice of moments reflects the basic stylized facts of daily returns of a stock market index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cython\n",
    "\n",
    "# import numpy as np\n",
    "# import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from yahoofinancials import YahooFinancials\n",
    "import scipy.optimize as opt\n",
    "from arch import arch_model\n",
    "from scipy.stats import kurtosis, skew\n",
    "import matplotlib as mpl\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "import os, pickle as pkl\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "from itertools import product\n",
    "\n",
    "LAG_1_AUTOCORRELATION_RAW = 'Lag-1 Autocorrelation (raw)'\n",
    "MEAN_ABS = 'Mean (absolute)' \n",
    "HILL_ESTIMATE = 'Hill estimator (absolute)' \n",
    "LAG_1_AUTOCORRELATION_ABS = 'Lag-1 Autocorrelation (absolute)'\n",
    "LAG_5_AUTOCORRELATION_ABS = 'Lag-5 Autocorrelation (absolute)'\n",
    "LAG_10_AUTOCORRELATION_ABS = 'Lag-10 Autocorrelation (absolute)'\n",
    "LAG_25_AUTOCORRELATION_ABS = 'Lag-25 Autocorrelation (absolute)'\n",
    "LAG_50_AUTOCORRELATION_ABS = 'Lag-50 Autocorrelation (absolute)'\n",
    "LAG_100_AUTOCORRELATION_ABS = 'Lag-100 Autocorrelation (absolute)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction successful....\n",
      "Summary: \n",
      "\n",
      "               ^GSPC\n",
      "count  10232.000000\n",
      "mean    1027.420476\n",
      "std      777.388608\n",
      "min       98.220001\n",
      "25%      333.962494\n",
      "50%     1006.740021\n",
      "75%     1390.694946\n",
      "max     3386.149902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction successful....\n",
      "Summary: \n",
      "\n",
      "               ^GSPC\n",
      "count  10232.000000\n",
      "mean    1027.420476\n",
      "std      777.388608\n",
      "min       98.220001\n",
      "25%      333.962494\n",
      "50%     1006.740021\n",
      "75%     1390.694946\n",
      "max     3386.149902\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-792de6fbe2ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cython'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--annotate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nimport numpy as np\\nimport time\\nimport argparse\\nimport pandas as pd\\nimport seaborn as sns\\nfrom statsmodels.tsa.stattools import acf\\nfrom yahoofinancials import YahooFinancials\\nimport scipy.optimize as opt\\nfrom arch import arch_model\\nfrom scipy.stats import kurtosis, skew\\nimport matplotlib as mpl\\nimport statsmodels.api as sm\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\nsns.set()\\nimport os, pickle as pkl\\nplt.rcParams[\\'figure.figsize\\'] = (15, 6)\\nfrom itertools import product\\n\\nLAG_1_AUTOCORRELATION_RAW = \\'Lag-1 Autocorrelation (raw)\\'\\nMEAN_ABS = \\'Mean (absolute)\\' \\nHILL_ESTIMATE = \\'Hill estimator (absolute)\\' \\nLAG_1_AUTOCORRELATION_ABS = \\'Lag-1 Autocorrelation (absolute)\\'\\nLAG_5_AUTOCORRELATION_ABS = \\'Lag-5 Autocorrelation (absolute)\\'\\nLAG_10_AUTOCORRELATION_ABS = \\'Lag-10 Autocorrelation (absolute)\\'\\nLAG_25_AUTOCORRELATION_ABS = \\'Lag-25 Autocorrelation (absolute)\\'\\nLAG_50_AUTOCORRELATION_ABS = \\'Lag-50 Autocorrelation (absolute)\\'\\nLAG_100_AUTOCORRELATION_ABS = \\'Lag-100 Autocorrelation (absolute)\\'\\n\\nclass DataLoader:\\n    def __init__(self, assets, start_date, end_date, frequency=\\'daily\\'):\\n        self.assets = assets\\n        self.start_date = start_date\\n        self.end_date = end_date\\n        self.frequency = frequency\\n        \\n    \\n    def get_empirical_data(self, log_returns = True, commentary=True):\\n        yahoo_financial = YahooFinancials(self.assets)\\n        try:\\n            data = yahoo_financial.get_historical_price_data(start_date=self.start_date,\\n                                                             end_date=self.end_date,\\n                                                             time_interval=self.frequency)\\n            df = pd.DataFrame({\\n            a: {x[\\'formatted_date\\']: x[\\'adjclose\\'] for x in data[a][\\'prices\\']} for a in assets\\n        })\\n        except:\\n            print(\"Not able to download data using yahoo financials. System will exit here.\")\\n            sys.exit(0)\\n            \\n        df = df.set_index(pd.DatetimeIndex(df.index))\\n        \\n        if log_returns:\\n            df = np.log(df / df.shift(1)).dropna()\\n            \\n        if commentary:\\n            print(\"Data extraction successful....\\\\nSummary: \\\\n\\\\n\", df.describe())\\n            \\n        return df\\n    \\n\\n        \\ndef calculate_series_return(series, log=False):\\n    if log:\\n        return (np.log(series) - np.log(series.shift(1))).dropna()\\n    else:\\n        return (series - series.shift(1)).dropna()\\n\\nclass Moments:\\n    @staticmethod\\n    def get_numerical_moments(float[:] series, int[:] moment_list, averaging_offset=None):\\n        print(\"AAYA yahan\")\\n        cdef int moment_max\\n        if averaging_offset is None:\\n            moment_max = np.max(moment_list)\\n        else:\\n            moment_max = np.max(moment_list) + averaging_offset\\n        \\n        autocorr = acf(series, unbiased=True, nlags=moment_max, fft=False)\\n        \\n        results = []\\n        \\n        if averaging_offset is None:\\n            results = [autocorr[moment] for moment in moment_list]\\n        else:\\n            results = [np.mean(autocorr[1:3]) if moment==1 \\\\\\n                       else np.mean(autocorr[moment-averaging_offset:moment + averaging_offset+1])\\\\\\n                      for moment in moment_list]\\n    \\n        return results\\n    @staticmethod\\n    def get_hill_estimator(series, k=0.05):\\n        abs_series = np.sort(np.abs(series))\\n        num_terms = int(k*len(abs_series))\\n        gamma = np.mean(np.log(abs_series[-num_terms:])) - abs_series[-(num_terms+1)]\\n        return 1.0/gamma\\n    \\n    \\nclass HPM:\\n    \\n    HPM_INIT = {\\n                    \"mu\": 0.01, \\n                    \"phi\": 0.12,\\n                    \"chi\": 1.5,\\n                    \"pstar\": 100, \\n                    \"sigma_f\": 0.758,\\n                    \"sigma_c\": 2.087 \\n                }\\n    \\n    \\n    def optimise_criterion(self, params_dict_values, actual_ts_moments, weighing_matrix, params_dict_keys, price0, ts_length, buffer_multiplier=5):\\n        \\n        # reconstruct params dict for simulation\\n        params_dict = {params_dict_keys[i]:params_dict_values[i] for i in range(len(params_dict_keys))}\\n\\n        \\n        # Calculating simulated price\\n        simulated_price = self.simulate(param_dict = params_dict, \\n                                        num_paths = 1, \\n                                        length=buffer_multiplier*ts_length, \\n                                        price0=price0)\\n        # Ignore first few points\\n        simulated_price = simulated_price[ts_length:, :]\\n        \\n        # Simulated price returns\\n        simulated_price_returns = np.log(simulated_price[1:, :]) - np.log(simulated_price[:-1, :])\\n        \\n        # Simulated time series moments \\n        simulated_ts_moments, _ = self.get_moments(simulated_price_returns[:, 0])\\n        \\n        # Error between simulated and actual moments \\n        error = np.array(actual_ts_moments).reshape(1, -1) - np.array(simulated_ts_moments).reshape(1, -1)\\n        \\n        return (error @ weighing_matrix @ error.T).flatten()[0]\\n    \\n    def calibrate(self, initial_params_dict, double[:] ts, weighing_matrix, price0, n_max_iter=1000, grid_init_search=True):\\n        # Separate keys and values \\n        keys, values = list(initial_params_dict.keys()), tuple(initial_params_dict.values())\\n\\n        if grid_init_search:\\n            minima = np.inf\\n            minima_dict = None\\n            for dictionary in self.get_shocked_dictionaries(initial_params_dict):\\n                error_value = self.optimise_criterion(dictionary, ts, weighing_matrix, keys, price0)\\n                if error_value <= minima:\\n                    minima_dict = dictionary.copy()\\n            \\n            init_params_dict = minima_dict.copy()\\n\\n        # Minimize the weighted error\\n        empirical_moments, _ = self.get_moments(ts)\\n        calibrated_result = opt.minimize(self.optimise_criterion, values, args=(empirical_moments, weighing_matrix, keys, price0, len(ts)),\\\\\\n                                        method=\\'Nelder-Mead\\', options={\\'maxiter\\': n_max_iter, \\'disp\\': True})\\n        \\n        # Calibrated parameters\\n        calibrated_params = {keys[i]:calibrated_result.x[i] for i in range(len(keys))}\\n        \\n        return calibrated_params\\n    \\n    \\n    def get_shocked_dictionaries(self, init_params_dict, shock=0.2, num_points=5):\\n        keys, values = list(init_params_dict.keys()), list(init_params_dict.values())\\n        values_modified = list(product(*[list(np.linspace((1-shock)*val, (1+shock)*val, num_points)) for val in values]))\\n        for val in values_modified:\\n            yield {keys[i]: val[i] for i in range(len(keys))}\\n            \\n    \\n    \\n    def get_moments(self, ts):\\n        \\'\\'\\'\\n        Returns the moments in the following order:\\n        [\\'Lag 1 autocorrelation from raw returns\\', \\'Mean absolute level\\', \\'Lag 1 autocorrelation from abs returns\\',\\n        \\'Lag 5 autocorrelation from abs returns\\', \\'Lag 10 autocorrelation from abs returns\\', \\n        \\'Lag 25 autocorrelation from abs returns\\', \\'Lag 50 autocorrelation from abs returns\\',\\n        \\'Lag 100 autocorrelation from abs returns\\', \\'Hill estimator\\']\\n        \\'\\'\\'\\n        \\n        \\n        m = []\\n        \\n        abs_ts = np.abs(ts)\\n        \\n        \\n        # First order autocorrelation coefficient from raw returns\\n        m = m + Moments.get_numerical_moments(ts, [1])\\n        \\n        # Mean of absolute returns \\n        m = m + [np.mean(abs_ts)]\\n        \\n        # Hill estimator \\n        m = m + [Moments.get_hill_estimator(abs_ts)]\\n        \\n        # lag [1,5] from absolute returns \\n        m = m + Moments.get_numerical_moments(abs_ts, [1, 5, 10, 25, 50, 100], averaging_offset=1)\\n        \\n        return m, [LAG_1_AUTOCORRELATION_RAW, MEAN_ABS, HILL_ESTIMATE, \\n                   LAG_1_AUTOCORRELATION_ABS, LAG_5_AUTOCORRELATION_ABS, \\n                   LAG_10_AUTOCORRELATION_ABS, LAG_25_AUTOCORRELATION_ABS, \\n                  LAG_50_AUTOCORRELATION_ABS, LAG_100_AUTOCORRELATION_ABS]\\n    \\n        \\n    def get_weighing_matrix(self, actual_ts, num_bootstraps=5000,  method = \\'block-bootstrap\\'):\\n        \\n        if method.lower() ==\\'block-bootstrap\\':\\n            \\n            moment_samples = np.zeros((9, num_bootstraps))\\n            abs_ts = np.abs(actual_ts)\\n            \\n            num_blocks_250 = len(actual_ts) // 250\\n            num_blocks_750 = len(actual_ts) // 750\\n            \\n            for i in range(num_bootstraps):\\n                \\n                m = []\\n                \\n                # numeric_moments_absolute lags = [1, 5, 10, 25, 100]\\n                modified_ts = np.array([])\\n                \\n                for j in range(num_blocks_250):\\n                    slice_start = np.random.randint(num_blocks_250)*250\\n                    modified_ts = np.append(modified_ts, actual_ts[slice_start:slice_start+250])\\n                \\n                m = self.get_moments(modified_ts)[0][:-4]\\n\\n                modified_abs_ts = []\\n                \\n                for j in range(num_blocks_750):\\n                    slice_start = np.random.randint(num_blocks_750)*750\\n                    modified_abs_ts = np.append(modified_abs_ts, abs_ts[slice_start:slice_start+750])\\n            \\n                m = m + self.get_moments(modified_abs_ts)[0][-4:]\\n                moment_samples[:, i] = m\\n                \\n            weighing_matrix = np.linalg.inv(np.cov(moment_samples))\\n            \\n        elif method.lower()==\\'equal\\':\\n            weighing_matrix = np.eye(9)\\n            \\n        return weighing_matrix\\n    \\n        \\n    \\n    def simulate(self, param_dict, num_paths, price0, length,  seed=1001):\\n        \\n        np.random.seed(seed)\\n        \\n        price = np.zeros((length+1, num_paths))\\n        demand_fundamentalist = np.zeros((length+1, num_paths))\\n        demand_chartist = np.zeros((length+1, num_paths))\\n        num_chartist = np.ones((length+1, num_paths))*0.5\\n        num_fundamentalist = np.ones((length+1, num_paths))*0.5\\n        \\n        price[0, :] = np.array([price0]*num_paths)\\n\\n        for t in range(1, length+1):\\n            price[t, :] = price[t-1, :] + param_dict[\\'mu\\']*(num_fundamentalist[t-1, :]*demand_fundamentalist[t-1, :] \\\\\\n                                                     + num_chartist[t-1, :]*demand_chartist[t-1, :])\\n            \\n            demand_fundamentalist[t, :] = param_dict[\\'phi\\']*(param_dict[\\'pstar\\'] - price[t,:]) \\\\\\n                                        + np.random.normal(scale= param_dict[\\'sigma_f\\'], size=num_paths)\\n            demand_chartist[t, :] = param_dict[\\'chi\\']*(price[t]-price[t-1]) \\\\\\n                                        + np.random.normal(scale= param_dict[\\'sigma_c\\'], size=num_paths)\\n             \\n        return price\\n    \\n    \\n    \\ndef yield_slices(series, slice_length, min_size=200):\\n    slice_index = 0\\n    while True:\\n        print(\"ys\")\\n        sliced_series = series[slice_length*slice_index:slice_length*(slice_index+1)]\\n        if len(sliced_series) >= min_size: \\n            yield sliced_series\\n            \\n        slice_index +=1\\n        \\n        \\ndef write_timer():\\n    elapsed_time = None\\n    \\n    while True:\\n        if elapsed_time is None:\\n            elapsed_time = 0\\n            last_time = time.time()\\n            yield \"\"\\n        else:\\n            elapsed_time = time.time() - last_time\\n            last_time = time.time()\\n            yield elapsed_time\\n            \\n\\n            \\n            \\n            \\nassets = [\\'^GSPC\\']\\nassets_dict = {\\'^GSPC\\':\\'SPX\\'}\\nstart_date = \\'1979-12-31\\'\\nend_date = \\'2020-07-29\\'\\nfrequency = \\'daily\\'\\n\\ndl = DataLoader(assets, start_date, end_date, frequency=frequency)\\ndf = dl.get_empirical_data(log_returns=False)\\n            \\nnum_paths = 1000\\nslice_length = 500\\nn_optimizer_iterations = 1000\\n\\ncdef double [:] asset_log_price_ts_view\\n\\nfor asset in assets:\\n    asset_log_price_ts = np.log(df[asset])\\n    asset_log_price_ts_view = asset_log_price_ts\\n    \\n    hpm_obj = HPM()\\n    \\n    for price_ts in yield_slices(asset_log_price_ts_view, slice_length, min_size=200):\\n        return_ts = calculate_series_return(price_ts)\\n        \\n        weighing_matrix = hpm_obj.get_weighing_matrix(return_ts, method=\\'equal\\')\\n        \\n        init_params_dict = HPM.HPM_INIT\\n        \\n        init_params_dict[\\'pstar\\'] = np.mean(return_ts)\\n        \\n        timer = write_timer(); next(timer)\\n        \\n        \\n        calibrated_params_dict = hpm_obj.calibrate(initial_params_dict = init_params_dict, \\n                                              ts = return_ts.values, \\n                                              weighing_matrix = weighing_matrix,\\n                                              price0= price_ts[0],\\n                                              n_max_iter = n_optimizer_iterations, \\n                                              grid_init_search=False)\\n        \\n        print(\"Time taken for calibration :\", next(timer))\\n        price_simulated = hpm_obj.simulate(calibrated_params_dict,\\n                                           num_paths=num_paths,\\n                                           price0= price_ts[0],\\n                                           length=5*len(asset_log_price_ts))\\n        \\n        print(\"Time taken for simulation :\", next(timer))\\n        price_simulated = price_simulated[len(price_ts):, :]\\n        \\n        returns_simulated = price_simulated[1:, :] - price_simulated[:-1, :]\\n        write_timer()\\n        moments_simulated = [hpm_obj.get_moments(returns_simulated[:, i])[0] for i in range(num_paths)]\\n        \\n        moments_actual, headers = hpm_obj.get_moments(return_ts)\\n        \\n        moments_simulated_df = pd.DataFrame(np.array(moments_simulated), columns=headers)\\n        \\n        print(\"Time taken to get simulated moments :\", next(timer))\\n        for header in headers:\\n            fig, ax = plt.subplots()\\n            ax.hist(moments_simulated_df[header], density=True, bins=50)\\n            ax.axvline(moments_actual[headers.index(header)], color=\\'red\\')\\n            plt.title(header + \" (Datapoints: \" + len(price_ts) + \")\")\\n            if not os.path.isdir(header):\\n                os.mkdir(header)\\n            plt.show()\\n            plt.savefig(os.path.join(header, price_ts.index[0].strftime(\"%Y-%m-%d\") + \"-\" + price_ts.index[-1].strftime(\"%Y-%m-%d\") + \\'.png\\'), format=\\'png\\')\\n        \\n        \\n        \\n        \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/manishchalana/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-158>\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/Cython/Build/IpythonMagic.py\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_import_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    340\u001b[0m         spec = importlib.machinery.ModuleSpec(\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m_cython_magic_ea5f058f4d7a6b601278230e35fc61a7.pyx\u001b[0m in \u001b[0;36minit _cython_magic_ea5f058f4d7a6b601278230e35fc61a7\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.ipython/cython/_cython_magic_ea5f058f4d7a6b601278230e35fc61a7.cpython-37m-darwin.so\u001b[0m in \u001b[0;36mView.MemoryView.memoryview_cwrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.ipython/cython/_cython_magic_ea5f058f4d7a6b601278230e35fc61a7.cpython-37m-darwin.so\u001b[0m in \u001b[0;36mView.MemoryView.memoryview.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'Series'"
     ]
    }
   ],
   "source": [
    "%%cython --annotate\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from yahoofinancials import YahooFinancials\n",
    "import scipy.optimize as opt\n",
    "from arch import arch_model\n",
    "from scipy.stats import kurtosis, skew\n",
    "import matplotlib as mpl\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sns.set()\n",
    "import os, pickle as pkl\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "from itertools import product\n",
    "\n",
    "LAG_1_AUTOCORRELATION_RAW = 'Lag-1 Autocorrelation (raw)'\n",
    "MEAN_ABS = 'Mean (absolute)' \n",
    "HILL_ESTIMATE = 'Hill estimator (absolute)' \n",
    "LAG_1_AUTOCORRELATION_ABS = 'Lag-1 Autocorrelation (absolute)'\n",
    "LAG_5_AUTOCORRELATION_ABS = 'Lag-5 Autocorrelation (absolute)'\n",
    "LAG_10_AUTOCORRELATION_ABS = 'Lag-10 Autocorrelation (absolute)'\n",
    "LAG_25_AUTOCORRELATION_ABS = 'Lag-25 Autocorrelation (absolute)'\n",
    "LAG_50_AUTOCORRELATION_ABS = 'Lag-50 Autocorrelation (absolute)'\n",
    "LAG_100_AUTOCORRELATION_ABS = 'Lag-100 Autocorrelation (absolute)'\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, assets, start_date, end_date, frequency='daily'):\n",
    "        self.assets = assets\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    \n",
    "    def get_empirical_data(self, log_returns = True, commentary=True):\n",
    "        yahoo_financial = YahooFinancials(self.assets)\n",
    "        try:\n",
    "            data = yahoo_financial.get_historical_price_data(start_date=self.start_date,\n",
    "                                                             end_date=self.end_date,\n",
    "                                                             time_interval=self.frequency)\n",
    "            df = pd.DataFrame({\n",
    "            a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in assets\n",
    "        })\n",
    "        except:\n",
    "            print(\"Not able to download data using yahoo financials. System will exit here.\")\n",
    "            sys.exit(0)\n",
    "            \n",
    "        df = df.set_index(pd.DatetimeIndex(df.index))\n",
    "        \n",
    "        if log_returns:\n",
    "            df = np.log(df / df.shift(1)).dropna()\n",
    "            \n",
    "        if commentary:\n",
    "            print(\"Data extraction successful....\\nSummary: \\n\\n\", df.describe())\n",
    "            \n",
    "        return df\n",
    "    \n",
    "\n",
    "        \n",
    "def calculate_series_return(series, log=False):\n",
    "    if log:\n",
    "        return (np.log(series) - np.log(series.shift(1))).dropna()\n",
    "    else:\n",
    "        return (series - series.shift(1)).dropna()\n",
    "\n",
    "class Moments:\n",
    "    @staticmethod\n",
    "    def get_numerical_moments(float[:] series, int[:] moment_list, averaging_offset=None):\n",
    "        print(\"AAYA yahan\")\n",
    "        cdef int moment_max\n",
    "        if averaging_offset is None:\n",
    "            moment_max = np.max(moment_list)\n",
    "        else:\n",
    "            moment_max = np.max(moment_list) + averaging_offset\n",
    "        \n",
    "        autocorr = acf(series, unbiased=True, nlags=moment_max, fft=False)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        if averaging_offset is None:\n",
    "            results = [autocorr[moment] for moment in moment_list]\n",
    "        else:\n",
    "            results = [np.mean(autocorr[1:3]) if moment==1 \\\n",
    "                       else np.mean(autocorr[moment-averaging_offset:moment + averaging_offset+1])\\\n",
    "                      for moment in moment_list]\n",
    "    \n",
    "        return results\n",
    "    @staticmethod\n",
    "    def get_hill_estimator(series, k=0.05):\n",
    "        abs_series = np.sort(np.abs(series))\n",
    "        num_terms = int(k*len(abs_series))\n",
    "        gamma = np.mean(np.log(abs_series[-num_terms:])) - abs_series[-(num_terms+1)]\n",
    "        return 1.0/gamma\n",
    "    \n",
    "    \n",
    "class HPM:\n",
    "    \n",
    "    HPM_INIT = {\n",
    "                    \"mu\": 0.01, \n",
    "                    \"phi\": 0.12,\n",
    "                    \"chi\": 1.5,\n",
    "                    \"pstar\": 100, \n",
    "                    \"sigma_f\": 0.758,\n",
    "                    \"sigma_c\": 2.087 \n",
    "                }\n",
    "    \n",
    "    \n",
    "    def optimise_criterion(self, params_dict_values, actual_ts_moments, weighing_matrix, params_dict_keys, price0, ts_length, buffer_multiplier=5):\n",
    "        \n",
    "        # reconstruct params dict for simulation\n",
    "        params_dict = {params_dict_keys[i]:params_dict_values[i] for i in range(len(params_dict_keys))}\n",
    "\n",
    "        \n",
    "        # Calculating simulated price\n",
    "        simulated_price = self.simulate(param_dict = params_dict, \n",
    "                                        num_paths = 1, \n",
    "                                        length=buffer_multiplier*ts_length, \n",
    "                                        price0=price0)\n",
    "        # Ignore first few points\n",
    "        simulated_price = simulated_price[ts_length:, :]\n",
    "        \n",
    "        # Simulated price returns\n",
    "        simulated_price_returns = np.log(simulated_price[1:, :]) - np.log(simulated_price[:-1, :])\n",
    "        \n",
    "        # Simulated time series moments \n",
    "        simulated_ts_moments, _ = self.get_moments(simulated_price_returns[:, 0])\n",
    "        \n",
    "        # Error between simulated and actual moments \n",
    "        error = np.array(actual_ts_moments).reshape(1, -1) - np.array(simulated_ts_moments).reshape(1, -1)\n",
    "        \n",
    "        return (error @ weighing_matrix @ error.T).flatten()[0]\n",
    "    \n",
    "    def calibrate(self, initial_params_dict, double[:] ts, weighing_matrix, price0, n_max_iter=1000, grid_init_search=True):\n",
    "        # Separate keys and values \n",
    "        keys, values = list(initial_params_dict.keys()), tuple(initial_params_dict.values())\n",
    "\n",
    "        if grid_init_search:\n",
    "            minima = np.inf\n",
    "            minima_dict = None\n",
    "            for dictionary in self.get_shocked_dictionaries(initial_params_dict):\n",
    "                error_value = self.optimise_criterion(dictionary, ts, weighing_matrix, keys, price0)\n",
    "                if error_value <= minima:\n",
    "                    minima_dict = dictionary.copy()\n",
    "            \n",
    "            init_params_dict = minima_dict.copy()\n",
    "\n",
    "        # Minimize the weighted error\n",
    "        empirical_moments, _ = self.get_moments(ts)\n",
    "        calibrated_result = opt.minimize(self.optimise_criterion, values, args=(empirical_moments, weighing_matrix, keys, price0, len(ts)),\\\n",
    "                                        method='Nelder-Mead', options={'maxiter': n_max_iter, 'disp': True})\n",
    "        \n",
    "        # Calibrated parameters\n",
    "        calibrated_params = {keys[i]:calibrated_result.x[i] for i in range(len(keys))}\n",
    "        \n",
    "        return calibrated_params\n",
    "    \n",
    "    \n",
    "    def get_shocked_dictionaries(self, init_params_dict, shock=0.2, num_points=5):\n",
    "        keys, values = list(init_params_dict.keys()), list(init_params_dict.values())\n",
    "        values_modified = list(product(*[list(np.linspace((1-shock)*val, (1+shock)*val, num_points)) for val in values]))\n",
    "        for val in values_modified:\n",
    "            yield {keys[i]: val[i] for i in range(len(keys))}\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_moments(self, ts):\n",
    "        '''\n",
    "        Returns the moments in the following order:\n",
    "        ['Lag 1 autocorrelation from raw returns', 'Mean absolute level', 'Lag 1 autocorrelation from abs returns',\n",
    "        'Lag 5 autocorrelation from abs returns', 'Lag 10 autocorrelation from abs returns', \n",
    "        'Lag 25 autocorrelation from abs returns', 'Lag 50 autocorrelation from abs returns',\n",
    "        'Lag 100 autocorrelation from abs returns', 'Hill estimator']\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        m = []\n",
    "        \n",
    "        abs_ts = np.abs(ts)\n",
    "        \n",
    "        \n",
    "        # First order autocorrelation coefficient from raw returns\n",
    "        m = m + Moments.get_numerical_moments(ts, [1])\n",
    "        \n",
    "        # Mean of absolute returns \n",
    "        m = m + [np.mean(abs_ts)]\n",
    "        \n",
    "        # Hill estimator \n",
    "        m = m + [Moments.get_hill_estimator(abs_ts)]\n",
    "        \n",
    "        # lag [1,5] from absolute returns \n",
    "        m = m + Moments.get_numerical_moments(abs_ts, [1, 5, 10, 25, 50, 100], averaging_offset=1)\n",
    "        \n",
    "        return m, [LAG_1_AUTOCORRELATION_RAW, MEAN_ABS, HILL_ESTIMATE, \n",
    "                   LAG_1_AUTOCORRELATION_ABS, LAG_5_AUTOCORRELATION_ABS, \n",
    "                   LAG_10_AUTOCORRELATION_ABS, LAG_25_AUTOCORRELATION_ABS, \n",
    "                  LAG_50_AUTOCORRELATION_ABS, LAG_100_AUTOCORRELATION_ABS]\n",
    "    \n",
    "        \n",
    "    def get_weighing_matrix(self, actual_ts, num_bootstraps=5000,  method = 'block-bootstrap'):\n",
    "        \n",
    "        if method.lower() =='block-bootstrap':\n",
    "            \n",
    "            moment_samples = np.zeros((9, num_bootstraps))\n",
    "            abs_ts = np.abs(actual_ts)\n",
    "            \n",
    "            num_blocks_250 = len(actual_ts) // 250\n",
    "            num_blocks_750 = len(actual_ts) // 750\n",
    "            \n",
    "            for i in range(num_bootstraps):\n",
    "                \n",
    "                m = []\n",
    "                \n",
    "                # numeric_moments_absolute lags = [1, 5, 10, 25, 100]\n",
    "                modified_ts = np.array([])\n",
    "                \n",
    "                for j in range(num_blocks_250):\n",
    "                    slice_start = np.random.randint(num_blocks_250)*250\n",
    "                    modified_ts = np.append(modified_ts, actual_ts[slice_start:slice_start+250])\n",
    "                \n",
    "                m = self.get_moments(modified_ts)[0][:-4]\n",
    "\n",
    "                modified_abs_ts = []\n",
    "                \n",
    "                for j in range(num_blocks_750):\n",
    "                    slice_start = np.random.randint(num_blocks_750)*750\n",
    "                    modified_abs_ts = np.append(modified_abs_ts, abs_ts[slice_start:slice_start+750])\n",
    "            \n",
    "                m = m + self.get_moments(modified_abs_ts)[0][-4:]\n",
    "                moment_samples[:, i] = m\n",
    "                \n",
    "            weighing_matrix = np.linalg.inv(np.cov(moment_samples))\n",
    "            \n",
    "        elif method.lower()=='equal':\n",
    "            weighing_matrix = np.eye(9)\n",
    "            \n",
    "        return weighing_matrix\n",
    "    \n",
    "        \n",
    "    \n",
    "    def simulate(self, param_dict, num_paths, price0, length,  seed=1001):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        price = np.zeros((length+1, num_paths))\n",
    "        demand_fundamentalist = np.zeros((length+1, num_paths))\n",
    "        demand_chartist = np.zeros((length+1, num_paths))\n",
    "        num_chartist = np.ones((length+1, num_paths))*0.5\n",
    "        num_fundamentalist = np.ones((length+1, num_paths))*0.5\n",
    "        \n",
    "        price[0, :] = np.array([price0]*num_paths)\n",
    "\n",
    "        for t in range(1, length+1):\n",
    "            price[t, :] = price[t-1, :] + param_dict['mu']*(num_fundamentalist[t-1, :]*demand_fundamentalist[t-1, :] \\\n",
    "                                                     + num_chartist[t-1, :]*demand_chartist[t-1, :])\n",
    "            \n",
    "            demand_fundamentalist[t, :] = param_dict['phi']*(param_dict['pstar'] - price[t,:]) \\\n",
    "                                        + np.random.normal(scale= param_dict['sigma_f'], size=num_paths)\n",
    "            demand_chartist[t, :] = param_dict['chi']*(price[t]-price[t-1]) \\\n",
    "                                        + np.random.normal(scale= param_dict['sigma_c'], size=num_paths)\n",
    "             \n",
    "        return price\n",
    "    \n",
    "    \n",
    "    \n",
    "def yield_slices(series, slice_length, min_size=200):\n",
    "    slice_index = 0\n",
    "    while True:\n",
    "        print(\"ys\")\n",
    "        sliced_series = series[slice_length*slice_index:slice_length*(slice_index+1)]\n",
    "        if len(sliced_series) >= min_size: \n",
    "            yield sliced_series\n",
    "            \n",
    "        slice_index +=1\n",
    "        \n",
    "        \n",
    "def write_timer():\n",
    "    elapsed_time = None\n",
    "    \n",
    "    while True:\n",
    "        if elapsed_time is None:\n",
    "            elapsed_time = 0\n",
    "            last_time = time.time()\n",
    "            yield \"\"\n",
    "        else:\n",
    "            elapsed_time = time.time() - last_time\n",
    "            last_time = time.time()\n",
    "            yield elapsed_time\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "assets = ['^GSPC']\n",
    "assets_dict = {'^GSPC':'SPX'}\n",
    "start_date = '1979-12-31'\n",
    "end_date = '2020-07-29'\n",
    "frequency = 'daily'\n",
    "\n",
    "dl = DataLoader(assets, start_date, end_date, frequency=frequency)\n",
    "df = dl.get_empirical_data(log_returns=False)\n",
    "            \n",
    "num_paths = 1000\n",
    "slice_length = 500\n",
    "n_optimizer_iterations = 1000\n",
    "\n",
    "cdef double [:] asset_log_price_ts_view\n",
    "\n",
    "for asset in assets:\n",
    "    asset_log_price_ts = np.log(df[asset])\n",
    "    asset_log_price_ts_view = asset_log_price_ts\n",
    "    \n",
    "    hpm_obj = HPM()\n",
    "    \n",
    "    for price_ts in yield_slices(asset_log_price_ts_view, slice_length, min_size=200):\n",
    "        return_ts = calculate_series_return(price_ts)\n",
    "        \n",
    "        weighing_matrix = hpm_obj.get_weighing_matrix(return_ts, method='equal')\n",
    "        \n",
    "        init_params_dict = HPM.HPM_INIT\n",
    "        \n",
    "        init_params_dict['pstar'] = np.mean(return_ts)\n",
    "        \n",
    "        timer = write_timer(); next(timer)\n",
    "        \n",
    "        \n",
    "        calibrated_params_dict = hpm_obj.calibrate(initial_params_dict = init_params_dict, \n",
    "                                              ts = return_ts.values, \n",
    "                                              weighing_matrix = weighing_matrix,\n",
    "                                              price0= price_ts[0],\n",
    "                                              n_max_iter = n_optimizer_iterations, \n",
    "                                              grid_init_search=False)\n",
    "        \n",
    "        print(\"Time taken for calibration :\", next(timer))\n",
    "        price_simulated = hpm_obj.simulate(calibrated_params_dict,\n",
    "                                           num_paths=num_paths,\n",
    "                                           price0= price_ts[0],\n",
    "                                           length=5*len(asset_log_price_ts))\n",
    "        \n",
    "        print(\"Time taken for simulation :\", next(timer))\n",
    "        price_simulated = price_simulated[len(price_ts):, :]\n",
    "        \n",
    "        returns_simulated = price_simulated[1:, :] - price_simulated[:-1, :]\n",
    "        write_timer()\n",
    "        moments_simulated = [hpm_obj.get_moments(returns_simulated[:, i])[0] for i in range(num_paths)]\n",
    "        \n",
    "        moments_actual, headers = hpm_obj.get_moments(return_ts)\n",
    "        \n",
    "        moments_simulated_df = pd.DataFrame(np.array(moments_simulated), columns=headers)\n",
    "        \n",
    "        print(\"Time taken to get simulated moments :\", next(timer))\n",
    "        for header in headers:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.hist(moments_simulated_df[header], density=True, bins=50)\n",
    "            ax.axvline(moments_actual[headers.index(header)], color='red')\n",
    "            plt.title(header + \" (Datapoints: \" + len(price_ts) + \")\")\n",
    "            if not os.path.isdir(header):\n",
    "                os.mkdir(header)\n",
    "            plt.show()\n",
    "            plt.savefig(os.path.join(header, price_ts.index[0].strftime(\"%Y-%m-%d\") + \"-\" + price_ts.index[-1].strftime(\"%Y-%m-%d\") + '.png'), format='png')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: \n",
    "\n",
    "In this approach, we assume rollinig slices of 2 year time points and weighing matrix is assumed to be identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = write_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(write_timer(\"haan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "gen = write_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if abraka is not None:\n",
    "    print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 107.94000244,  105.76000214,  105.22000122, ..., 3215.62988281,\n",
       "       3239.40991211, 3218.43994141])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[asset].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
