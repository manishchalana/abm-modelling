{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we implement the concept of structural stochastic volatility which derives from different noise levels in the demand of fundamentalist and chartists and time varying market share of the two groups. \n",
    "\n",
    "We consider the || name here || approach of endogenous switching between the trading strategies and estimate the model by method of simulated moments. where choice of moments reflects the basic stylized facts of daily returns of a stock market index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction successful....\n",
      "Summary: \n",
      "\n",
      "              ^GSPC\n",
      "count  1151.000000\n",
      "mean   2609.063177\n",
      "std     361.213298\n",
      "min    1829.079956\n",
      "25%    2341.890015\n",
      "50%    2663.419922\n",
      "75%    2879.630005\n",
      "max    3386.149902\n",
      "***** Running for time period : 2015-12-31 - 2017-12-22 *****\n",
      "Time taken to retrive weighing matrix 2.5987625122070312e-05\n",
      "\n",
      "Searching for optimised starting values....\n",
      "Reduced error from  inf to 1.1630556975639659\n",
      "Reduced error from  1.1630556975639659 to 0.7057809974826492\n",
      "Reduced error from  0.7057809974826492 to 0.6970286786923612\n",
      "Reduced error from  0.6970286786923612 to 0.6969098199506797\n",
      "Reduced error from  0.6969098199506797 to 0.6967909813993147\n",
      "Reduced error from  0.6967909813993147 to 0.6096546226323797\n",
      "Reduced error from  0.6096546226323797 to 0.4720393342612843\n",
      "Reduced error from  0.4720393342612843 to 0.36662777120899803\n",
      "Reduced error from  0.36662777120899803 to 0.35614544702588596\n",
      "Reduced error from  0.35614544702588596 to 0.345896085001731\n"
     ]
    }
   ],
   "source": [
    "%%cython \n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from yahoofinancials import YahooFinancials\n",
    "import scipy.optimize as opt\n",
    "from arch import arch_model\n",
    "from scipy.stats import kurtosis, skew\n",
    "import matplotlib as mpl\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sns.set()\n",
    "import os, pickle as pkl\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "from itertools import product\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "LAG_1_AUTOCORRELATION_RAW = 'Lag-1 Autocorrelation (raw)'\n",
    "MEAN_ABS = 'Mean (absolute)' \n",
    "HILL_ESTIMATE = 'Hill estimator (absolute)' \n",
    "LAG_1_AUTOCORRELATION_ABS = 'Lag-1 Autocorrelation (absolute)'\n",
    "LAG_5_AUTOCORRELATION_ABS = 'Lag-5 Autocorrelation (absolute)'\n",
    "LAG_10_AUTOCORRELATION_ABS = 'Lag-10 Autocorrelation (absolute)'\n",
    "LAG_25_AUTOCORRELATION_ABS = 'Lag-25 Autocorrelation (absolute)'\n",
    "LAG_50_AUTOCORRELATION_ABS = 'Lag-50 Autocorrelation (absolute)'\n",
    "LAG_100_AUTOCORRELATION_ABS = 'Lag-100 Autocorrelation (absolute)'\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, assets, start_date, end_date, frequency='daily'):\n",
    "        self.assets = assets\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    \n",
    "    def get_empirical_data(self, log_returns = True, commentary=True):\n",
    "        yahoo_financial = YahooFinancials(self.assets)\n",
    "        try:\n",
    "            data = yahoo_financial.get_historical_price_data(start_date=self.start_date,\n",
    "                                                             end_date=self.end_date,\n",
    "                                                             time_interval=self.frequency)\n",
    "            df = pd.DataFrame({\n",
    "            a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in assets\n",
    "        })\n",
    "        except:\n",
    "            print(\"Not able to download data using yahoo financials. System will exit here.\")\n",
    "            sys.exit(0)\n",
    "            \n",
    "        df = df.set_index(pd.DatetimeIndex(df.index))\n",
    "        \n",
    "        if log_returns:\n",
    "            df = np.log(df / df.shift(1)).dropna()\n",
    "            \n",
    "        if commentary:\n",
    "            print(\"Data extraction successful....\\nSummary: \\n\\n\", df.describe())\n",
    "            \n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "class Moments:\n",
    "    @staticmethod\n",
    "    def get_numerical_moments(series, moment_list, averaging_offset=None):\n",
    "\n",
    "        cdef int moment_max\n",
    "        if averaging_offset is None:\n",
    "            moment_max = np.max(moment_list)\n",
    "        else:\n",
    "            moment_max = np.max(moment_list) + averaging_offset\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        if averaging_offset is None:\n",
    "            results = [np.corrcoef(series[:-moment], series[moment:])[0,1] for moment in moment_list]\n",
    "        else:\n",
    "            results = [np.mean([np.corrcoef(series[:-mom], series[mom:])[0,1] for mom in range(1,averaging_offset+2)]) if moment==1 \\\n",
    "                       else np.mean([np.corrcoef(series[:-mom], series[mom:])[0,1] for mom in range(moment-averaging_offset,moment + averaging_offset+1)])\\\n",
    "                      for moment in moment_list]\n",
    "\n",
    "    \n",
    "        return results\n",
    "    @staticmethod\n",
    "    def get_hill_estimator(series, k=0.05):\n",
    "        abs_series = np.sort(np.abs(series))\n",
    "        num_terms = int(k*len(abs_series))\n",
    "        gamma = np.mean(np.log(abs_series[-num_terms:])) - np.log(abs_series[-(num_terms+1)])\n",
    "        return 1.0/gamma\n",
    "    \n",
    "    \n",
    "class HPM:\n",
    "    \n",
    "    HPM_INIT = {\n",
    "                    \"mu\": 0.01, \n",
    "                    \"phi\": 0.12,\n",
    "                    \"chi\": 1.5,\n",
    "                    \"pstar\": 100, \n",
    "                    \"sigma_f\": 0.758,\n",
    "                    \"sigma_c\": 2.087 \n",
    "                }\n",
    "    \n",
    "    \n",
    "    def optimise_criterion(self, params_dict_values, actual_ts_moments, weighing_matrix, params_dict_keys, price0, ts_length, buffer_multiplier=5):\n",
    "        \n",
    "        # reconstruct params dict for simulation\n",
    "        params_dict = {params_dict_keys[i]:params_dict_values[i] for i in range(len(params_dict_keys))}\n",
    "\n",
    "        \n",
    "        # Calculating simulated price\n",
    "        simulated_price = self.simulate(param_dict = params_dict, \n",
    "                                        num_paths = 1, \n",
    "                                        length=buffer_multiplier*ts_length, \n",
    "                                        price0=price0)\n",
    "        # Ignore first few points\n",
    "        simulated_price = simulated_price[ts_length:, :]\n",
    "        \n",
    "        # Simulated price returns\n",
    "        simulated_price_returns = np.log(simulated_price[1:, :]) - np.log(simulated_price[:-1, :])\n",
    "        \n",
    "        # Simulated time series moments \n",
    "        simulated_ts_moments, _ = self.get_moments(simulated_price_returns[:, 0])\n",
    "        \n",
    "        # Error between simulated and actual moments \n",
    "        error = np.array(actual_ts_moments).reshape(1, -1) - np.array(simulated_ts_moments).reshape(1, -1)\n",
    "        \n",
    "        return (error @ weighing_matrix @ error.T).flatten()[0]\n",
    "    \n",
    "    def calibrate(self, initial_params_dict, ts, weighing_matrix, price0, n_max_iter=1000, grid_init_search=True):\n",
    "        # Separate keys and values \n",
    "        keys, values = list(initial_params_dict.keys()), tuple(initial_params_dict.values())\n",
    "        \n",
    "        # Minimize the weighted error\n",
    "        empirical_moments, empirical_headers = self.get_moments(ts)\n",
    "        \n",
    "        if grid_init_search:\n",
    "            print(\"\\nSearching for optimised starting values....\")\n",
    "            minima = np.inf\n",
    "            minima_dict = None\n",
    "            for dictionary in self.get_shocked_dictionaries(initial_params_dict):\n",
    "                keys_dummy, values_dummy = list(dictionary.keys()), tuple(dictionary.values())\n",
    "                error_value = self.optimise_criterion(values_dummy, empirical_moments, weighing_matrix, keys_dummy, price0, len(ts))\n",
    "                if error_value <= minima:\n",
    "                    print(\"Reduced error from \", minima, \"to\", error_value)\n",
    "                    minima_dict = dictionary.copy()\n",
    "                    minima = error_value\n",
    "            \n",
    "            print(\"Initial dict: \", initial_params_dict)\n",
    "            initial_params_dict = minima_dict.copy()\n",
    "            print(\"Updated dict: \", initial_params_dict)\n",
    "            \n",
    "            # Separate keys and values \n",
    "            keys, values = list(initial_params_dict.keys()), tuple(initial_params_dict.values())\n",
    "\n",
    "        \n",
    "        \n",
    "        calibrated_result = opt.minimize(self.optimise_criterion, values, args=(empirical_moments, weighing_matrix, keys, price0, len(ts)),\\\n",
    "                                        method='Nelder-Mead', options={'maxiter': n_max_iter, 'disp': True})\n",
    "        \n",
    "        # Calibrated parameters\n",
    "        calibrated_params = {keys[i]:calibrated_result.x[i] for i in range(len(keys))}\n",
    "        \n",
    "        return calibrated_params, initial_params_dict\n",
    "    \n",
    "    \n",
    "    def get_shocked_dictionaries(self, init_params_dict, shock=0.2, num_points=3):\n",
    "        keys, values = list(init_params_dict.keys()), list(init_params_dict.values())\n",
    "        values_modified = list(product(*[list(np.linspace((1-shock)*val, (1+shock)*val, num_points)) for val in values]))\n",
    "        for val in values_modified:\n",
    "            yield {keys[i]: val[i] for i in range(len(keys))}\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_moments(self, ts, commentary=False):\n",
    "        '''\n",
    "        Returns the moments in the following order:\n",
    "        ['Lag 1 autocorrelation from raw returns', 'Mean absolute level', 'Lag 1 autocorrelation from abs returns',\n",
    "        'Lag 5 autocorrelation from abs returns', 'Lag 10 autocorrelation from abs returns', \n",
    "        'Lag 25 autocorrelation from abs returns', 'Lag 50 autocorrelation from abs returns',\n",
    "        'Lag 100 autocorrelation from abs returns', 'Hill estimator']\n",
    "        '''\n",
    "        if commentary:\n",
    "            print(\"ts :\", ts)\n",
    "        \n",
    "        m = []\n",
    "        \n",
    "        abs_ts = np.abs(ts)\n",
    "        \n",
    "        \n",
    "        # First order autocorrelation coefficient from raw returns\n",
    "\n",
    "        m = m + Moments.get_numerical_moments(ts, [1])\n",
    "        \n",
    "        # Mean of absolute returns \n",
    "        m = m + [np.mean(abs_ts)]\n",
    "        \n",
    "        \n",
    "        # Hill estimator \n",
    "        m = m + [Moments.get_hill_estimator(abs_ts)]\n",
    "\n",
    "        \n",
    "        # lag [1,5] from absolute returns \n",
    "        m = m + Moments.get_numerical_moments(abs_ts, [1, 5, 10, 25, 50, 100], averaging_offset=1)\n",
    "        \n",
    "        return m, [LAG_1_AUTOCORRELATION_RAW, MEAN_ABS, HILL_ESTIMATE, \n",
    "                   LAG_1_AUTOCORRELATION_ABS, LAG_5_AUTOCORRELATION_ABS, \n",
    "                   LAG_10_AUTOCORRELATION_ABS, LAG_25_AUTOCORRELATION_ABS, \n",
    "                  LAG_50_AUTOCORRELATION_ABS, LAG_100_AUTOCORRELATION_ABS]\n",
    "    \n",
    "        \n",
    "    def get_weighing_matrix(self, actual_ts, num_bootstraps=5000,  method = 'block-bootstrap'):\n",
    "        \n",
    "        if method.lower() =='block-bootstrap':\n",
    "            print(\"Actual ts :\", actual_ts)\n",
    "            \n",
    "            moment_samples = np.zeros((9, num_bootstraps))\n",
    "            abs_ts = np.abs(actual_ts)\n",
    "            \n",
    "            num_blocks_250 = len(actual_ts) // 250\n",
    "            num_blocks_750 = len(actual_ts) // 750\n",
    "            \n",
    "            blocks_250 = [actual_ts[j*250:(j+1)*250] for j in range(num_blocks_250)]\n",
    "            blocks_750 = [actual_ts[j*750:(j+1)*750] for j in range(num_blocks_750)]\n",
    "            \n",
    "            for i in range(num_bootstraps):\n",
    "                \n",
    "                m = []\n",
    "                \n",
    "                modified_ts = list(chain.from_iterable(random.choices(blocks_250, k=num_blocks_250)))\n",
    "                \n",
    "                m = self.get_moments(modified_ts)[0][:-4]\n",
    "        \n",
    "                modified_abs_ts = list(chain.from_iterable(random.choices(blocks_750, k=num_blocks_750)))\n",
    "            \n",
    "                m = m + self.get_moments(modified_abs_ts)[0][-4:]\n",
    "                \n",
    "                moment_samples[:, i] = m\n",
    "                \n",
    "            weighing_matrix = np.linalg.inv(np.cov(moment_samples))\n",
    "            \n",
    "        elif method.lower()=='equal':\n",
    "            weighing_matrix = np.eye(9)\n",
    "            \n",
    "        return weighing_matrix\n",
    "    \n",
    "        \n",
    "    \n",
    "    def simulate(self, param_dict, num_paths, price0, length,  seed=1001):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        cdef np.ndarray price = np.zeros((length+1, num_paths), dtype=np.float)\n",
    "        cdef np.ndarray demand_fundamentalist = np.zeros((length+1, num_paths), dtype=np.float)\n",
    "        cdef np.ndarray demand_chartist = np.zeros((length+1, num_paths), dtype=np.float)\n",
    "        cdef np.ndarray num_chartist = np.ones((length+1, num_paths), dtype=np.float )*0.5\n",
    "        cdef np.ndarray num_fundamentalist = np.ones((length+1, num_paths), dtype=np.float)*0.5\n",
    "        \n",
    "        price[0, :] = np.array([price0]*num_paths)\n",
    "\n",
    "        for t in range(1, length+1):\n",
    "            price[t, :] = price[t-1, :] + param_dict['mu']*(num_fundamentalist[t-1, :]*demand_fundamentalist[t-1, :] \\\n",
    "                                                     + num_chartist[t-1, :]*demand_chartist[t-1, :])\n",
    "            \n",
    "            demand_fundamentalist[t, :] = param_dict['phi']*(param_dict['pstar'] - price[t,:]) \\\n",
    "                                        + np.random.normal(scale= param_dict['sigma_f'], size=num_paths)\n",
    "            demand_chartist[t, :] = param_dict['chi']*(price[t]-price[t-1]) \\\n",
    "                                        + np.random.normal(scale= param_dict['sigma_c'], size=num_paths)\n",
    "             \n",
    "        return price\n",
    "    \n",
    "    \n",
    "    \n",
    "def yield_slices(series, slice_length, min_size=200):\n",
    "    slice_index = 0\n",
    "    while True:\n",
    "        sliced_series = series[slice_length*slice_index:slice_length*(slice_index+1)]\n",
    "        if len(sliced_series) >= min_size: \n",
    "            print(sliced_series)\n",
    "            yield sliced_series\n",
    "        slice_index +=1\n",
    "        \n",
    "def calculate_series_return(series, log=False):\n",
    "#     print(series[1:])\n",
    "    if log:\n",
    "        return (np.log(series) - np.log(series.shift(1))).dropna()\n",
    "    else:\n",
    "        return (series - series.shift(1)).dropna()\n",
    "#         return np.subtract(series[1:], series[:-1])\n",
    "\n",
    "\n",
    "    \n",
    "def write_timer():\n",
    "    elapsed_time = None\n",
    "    \n",
    "    while True:\n",
    "        if elapsed_time is None:\n",
    "            elapsed_time = 0\n",
    "            last_time = time.time()\n",
    "            yield \"\"\n",
    "        else:\n",
    "            elapsed_time = time.time() - last_time\n",
    "            last_time = time.time()\n",
    "            yield elapsed_time\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "assets = ['^GSPC']\n",
    "assets_dict = {'^GSPC':'SPX'}\n",
    "start_date = '2015-12-31'\n",
    "end_date = '2020-07-29'\n",
    "frequency = 'daily'\n",
    "parent_directory = start_date + \" to \" + end_date + \" (without initial grid search)\"\n",
    "if not os.path.isdir(parent_directory):\n",
    "    os.mkdir(parent_directory)\n",
    "    \n",
    "\n",
    "filename = os.path.join(parent_directory, (\"_\".join([assets_dict[asset] for asset in assets_dict])) + start_date + \"--\" + end_date + \" (\" + frequency + \").xlsx\" )   \n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(\"Reading data from file\")\n",
    "    df = pd.read_excel(filename, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    dl = DataLoader(assets, start_date, end_date, frequency=frequency)\n",
    "    df = dl.get_empirical_data(log_returns=False)\n",
    "    df.index.name='Date'\n",
    "    df.to_excel(filename)\n",
    "            \n",
    "num_paths = 5000\n",
    "slice_length = 500\n",
    "min_slice_length = 250 # In case not too many points are left\n",
    "n_optimizer_iterations = 1000\n",
    "moments_simulated_df_list = []\n",
    "calibrated_dicts_list = []\n",
    "initial_dicts_list = []\n",
    "\n",
    "for asset in assets:\n",
    "    asset_log_price_ts = np.log(df[asset])\n",
    "\n",
    "    \n",
    "    hpm_obj = HPM()\n",
    "    slice_index = 0\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        price_ts = asset_log_price_ts[slice_length*slice_index:slice_length*(slice_index+1)]\n",
    "        slice_index +=1\n",
    "        \n",
    "        if len(price_ts) <= min_slice_length: \n",
    "            break \n",
    "        print(\"*\"*5, \"Running for time period :\", price_ts.index[0].strftime(\"%Y-%m-%d\") + \" - \" + price_ts.index[-1].strftime(\"%Y-%m-%d\"), \"*\"*5)\n",
    "        \n",
    "        return_ts = calculate_series_return(price_ts)\n",
    "        \n",
    "        timer = write_timer(); next(timer)\n",
    "    \n",
    "        weighing_matrix = hpm_obj.get_weighing_matrix(return_ts, method='equal')\n",
    "        print(\"Time taken to retrive weighing matrix\", next(timer))\n",
    "        \n",
    "        init_params_dict = HPM.HPM_INIT\n",
    "        init_params_dict['pstar'] = np.mean(return_ts)\n",
    "        \n",
    "        calibrated_params_dict, initial_params_dict_grid_search  = hpm_obj.calibrate(initial_params_dict = init_params_dict, \n",
    "                                                                          ts = return_ts.values, \n",
    "                                                                          weighing_matrix = weighing_matrix,\n",
    "                                                                          price0= price_ts[0],\n",
    "                                                                          n_max_iter = n_optimizer_iterations, \n",
    "                                                                          grid_init_search=True)\n",
    "        \n",
    "        \n",
    "        print(\"Time taken for calibration :\", next(timer))\n",
    "        price_simulated = hpm_obj.simulate(calibrated_params_dict,\n",
    "                                           num_paths=num_paths,\n",
    "                                           price0= price_ts[0],\n",
    "                                           length=5*len(asset_log_price_ts))\n",
    "        \n",
    "        print(\"Time taken for simulation :\", next(timer))\n",
    "        price_simulated = price_simulated[len(price_ts):, :]\n",
    "        \n",
    "        returns_simulated = price_simulated[1:, :] - price_simulated[:-1, :]\n",
    "\n",
    "        moments_simulated = [hpm_obj.get_moments(returns_simulated[:, i])[0] for i in range(num_paths)]\n",
    "        \n",
    "        moments_actual, headers = hpm_obj.get_moments(return_ts, commentary=False)\n",
    "        \n",
    "        moments_simulated_df = pd.DataFrame(np.array(moments_simulated), columns=headers)\n",
    "        \n",
    "        print(\"Time taken to get simulated moments :\", next(timer))\n",
    "        for header in headers:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.hist(moments_simulated_df[header], density=True, bins=50, color='purple')\n",
    "            ax.axvline(moments_actual[headers.index(header)], color='red', linewidth=2)\n",
    "            plt.title(header + \" (Datapoints: \" + str(len(price_ts)) + \", Daterange: \" + price_ts.index[0].strftime(\"%Y-%m-%d\") + \"-\" + price_ts.index[-1].strftime(\"%Y-%m-%d\") + \")\")\n",
    "            if not os.path.isdir(os.path.join(parent_directory, header)):\n",
    "                os.mkdir(os.path.join(parent_directory, header))\n",
    "            plt.savefig(os.path.join(parent_directory, header, price_ts.index[0].strftime(\"%Y-%m-%d\") + \"-\" + price_ts.index[-1].strftime(\"%Y-%m-%d\") + '.jpg'), format='jpeg')\n",
    "            plt.show()\n",
    "            \n",
    "        moments_simulated_df['Start Date'] = price_ts.index[0]\n",
    "        moments_simulated_df['End Date'] = price_ts.index[-1]\n",
    "        \n",
    "        for header in headers:\n",
    "            moments_simulated_df[header + \"_Empirical\"] = moments_actual[headers.index(header)]\n",
    "                                                                         \n",
    "        moments_simulated_df_list.append(moments_simulated_df)\n",
    "        \n",
    "        calibrated_dicts_list.append(calibrated_params_dict)\n",
    "        initial_dicts_list.append(initial_params_dict_grid_search)\n",
    "        \n",
    "        \n",
    "    moments_simulated_combined = pd.concat(moments_simulated_df_list)\n",
    "\n",
    "\n",
    "    for header in headers:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(moments_simulated_df[header], density=True, bins=100, color='purple')\n",
    "        ax.hist(moments_simulated_df[header + \"_Empirical\"], density=True, bins=20, color='red')\n",
    "        plt.title(header + \" : Simulated vs Empirical\")\n",
    "        plt.savefig(os.path.join(parent_directory, header + \" - Simulated vs Empirical\" + '.jpg'), format='jpeg')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    calibrated_dicts_df = pd.DataFrame(calibrated_dicts_list)\n",
    "    initial_dicts_df = pd.DataFrame(initial_dicts_list)\n",
    "    \n",
    "    initial_dicts_df.rename(lambda x: x+ \"_Initial\", axis='columns')\n",
    "    \n",
    "    combined_dicts_df = pd.concat([calibrated_dicts_df, initial_dicts_df], axis=1)\n",
    "    \n",
    "    combined_dicts_df.to_excel(os.path.join(parent_directory, key + \" - Initial vs Final.xlsx\"))\n",
    "    \n",
    "    for key in calibrated_params_dict.keys():\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(combined_dicts_df.index, combined_dicts_df[key], color='purple', label=\"Final\")\n",
    "        ax.scatter(combined_dicts_df.index, combined_dicts_df[key + \"_Initial\"], color='red', label=\"Initial\")\n",
    "        ax.legend()\n",
    "        plt.title(key + \" : Initial vs Final\")\n",
    "        plt.savefig(os.path.join(parent_directory, key + \" - Initial vs Final.jpg\"), format='jpeg')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
